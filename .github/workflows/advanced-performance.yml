name: âš¡ Advanced Performance Testing

on:
  push:
    branches: [main]
    paths:
      - 'services/**'
      - 'infrastructure/**'
  pull_request:
    branches: [main]
    paths:
      - 'services/**'
      - 'infrastructure/**'
  schedule:
    # Run comprehensive performance tests weekly
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Performance test suite to run'
        type: choice
        options:
          - comprehensive
          - load-test
          - stress-test
          - spike-test
          - endurance-test
          - memory-leak-test
          - concurrent-user-test
        default: 'comprehensive'
      target_environment:
        description: 'Target environment'
        type: choice
        options:
          - local
          - staging
          - production-mirror
        default: 'staging'
      duration_minutes:
        description: 'Test duration in minutes'
        type: number
        default: 30
      concurrent_users:
        description: 'Number of concurrent users'
        type: number
        default: 500

env:
  PYTHON_VERSION: '3.12.9'
  NODE_VERSION: '18'

jobs:
  # Environment Setup
  setup-test-environment:
    name: ğŸ—ï¸ Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      environment-url: ${{ steps.setup.outputs.url }}
      environment-ready: ${{ steps.health.outputs.ready }}

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ³ Setup Docker Environment
        run: |
          docker-compose -f docker-compose.yml -f docker-compose.override.yml up -d
          sleep 30

      - name: ğŸ—ï¸ Environment Setup
        id: setup
        run: |
          echo "url=http://localhost:8000" >> $GITHUB_OUTPUT

      - name: ğŸ” Health Check
        id: health
        run: |
          max_attempts=30
          attempt=1

          while [ $attempt -le $max_attempts ]; do
            if curl -f http://localhost:8000/health > /dev/null 2>&1; then
              echo "ready=true" >> $GITHUB_OUTPUT
              echo "âœ… Environment is ready"
              exit 0
            fi
            
            echo "Attempt $attempt/$max_attempts failed, waiting..."
            sleep 10
            attempt=$((attempt + 1))
          done

          echo "ready=false" >> $GITHUB_OUTPUT
          echo "âŒ Environment failed to start"
          exit 1

  # Load Testing
  load-testing:
    name: ğŸ“ˆ Load Testing
    runs-on: ubuntu-latest
    needs: setup-test-environment
    if: needs.setup-test-environment.outputs.environment-ready == 'true' && (github.event.inputs.test_suite == 'load-test' || github.event.inputs.test_suite == 'comprehensive' || github.event.inputs.test_suite == '')

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Load Testing Tools
        run: |
          pip install locust pytest-benchmark requests aiohttp websockets

      - name: ğŸš€ Run Load Tests
        run: |
          echo "ğŸ”¥ Starting load tests..."

          # API Load Test
          locust --headless --users ${{ github.event.inputs.concurrent_users || 500 }} \
                 --spawn-rate 10 \
                 --run-time ${{ github.event.inputs.duration_minutes || 30 }}m \
                 --host http://localhost:8000 \
                 --html load-test-report.html \
                 --csv load-test-results \
                 -f tests/performance/locustfiles/api_load_test.py

      - name: ğŸ“Š Upload Load Test Results
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results_*.csv

  # Stress Testing
  stress-testing:
    name: ğŸ’ª Stress Testing
    runs-on: ubuntu-latest
    needs: setup-test-environment
    if: needs.setup-test-environment.outputs.environment-ready == 'true' && (github.event.inputs.test_suite == 'stress-test' || github.event.inputs.test_suite == 'comprehensive')

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Stress Testing Tools
        run: |
          pip install locust psutil matplotlib seaborn

      - name: ğŸ’ª Run Stress Tests
        run: |
          echo "ğŸ’¥ Starting stress tests..."

          # Gradually increase load until system breaks
          python tests/performance/stress_test.py \
            --target-url http://localhost:8000 \
            --max-users 2000 \
            --duration 600 \
            --output stress-test-results.json

      - name: ğŸ“Š Generate Stress Test Report
        run: |
          python tests/performance/generate_report.py \
            --input stress-test-results.json \
            --output stress-test-report.html \
            --type stress

      - name: ğŸ“Š Upload Stress Test Results
        uses: actions/upload-artifact@v3
        with:
          name: stress-test-results
          path: |
            stress-test-report.html
            stress-test-results.json

  # Memory and Resource Testing
  resource-testing:
    name: ğŸ§  Memory & Resource Testing
    runs-on: ubuntu-latest
    needs: setup-test-environment
    if: needs.setup-test-environment.outputs.environment-ready == 'true' && (github.event.inputs.test_suite == 'memory-leak-test' || github.event.inputs.test_suite == 'comprehensive')

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Monitoring Tools
        run: |
          pip install psutil memory-profiler py-spy docker

      - name: ğŸ§  Run Memory Tests
        run: |
          echo "ğŸ§  Starting memory and resource tests..."

          # Monitor memory usage over time
          python tests/performance/memory_test.py \
            --duration ${{ github.event.inputs.duration_minutes || 30 }} \
            --interval 30 \
            --output memory-usage.json

          # Profile memory usage of services
          py-spy record -o memory-profile.svg -d 300 -- python -c "import time; time.sleep(300)"

      - name: ğŸ“Š Generate Resource Report
        run: |
          python tests/performance/resource_report.py \
            --memory-data memory-usage.json \
            --output resource-report.html

      - name: ğŸ“Š Upload Resource Test Results
        uses: actions/upload-artifact@v3
        with:
          name: resource-test-results
          path: |
            resource-report.html
            memory-usage.json
            memory-profile.svg

  # Concurrent User Testing
  concurrent-testing:
    name: ğŸ‘¥ Concurrent User Testing
    runs-on: ubuntu-latest
    needs: setup-test-environment
    if: needs.setup-test-environment.outputs.environment-ready == 'true' && (github.event.inputs.test_suite == 'concurrent-user-test' || github.event.inputs.test_suite == 'comprehensive')

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Concurrency Testing Tools
        run: |
          pip install asyncio aiohttp websockets concurrent.futures

      - name: ğŸ‘¥ Run Concurrent User Tests
        run: |
          echo "ğŸ‘¥ Starting concurrent user tests..."

          # Test various concurrent user scenarios
          python tests/performance/concurrent_test.py \
            --base-url http://localhost:8000 \
            --scenarios tests/performance/scenarios/concurrent_scenarios.yaml \
            --output concurrent-test-results.json

      - name: ğŸ“Š Upload Concurrent Test Results
        uses: actions/upload-artifact@v3
        with:
          name: concurrent-test-results
          path: concurrent-test-results.json

  # Performance Report Generation
  generate-performance-report:
    name: ğŸ“Š Generate Performance Report
    runs-on: ubuntu-latest
    needs: [load-testing, stress-testing, resource-testing, concurrent-testing]
    if: always()

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ“¥ Download All Test Results
        uses: actions/download-artifact@v3

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ğŸ“¦ Install Report Generation Tools
        run: |
          pip install jinja2 matplotlib seaborn plotly pandas

      - name: ğŸ“Š Generate Comprehensive Report
        run: |
          echo "ğŸ“Š Generating comprehensive performance report..."

          python tests/performance/generate_comprehensive_report.py \
            --load-results load-test-results/ \
            --stress-results stress-test-results/ \
            --resource-results resource-test-results/ \
            --concurrent-results concurrent-test-results/ \
            --output performance-report.html

      - name: ğŸ“Š Upload Performance Report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: performance-report.html

      - name: ğŸ’¬ Comment Performance Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## âš¡ Performance Test Results\n\nğŸ“Š Comprehensive performance report generated!\n\n**Test Results:**\n- Load Testing: ${{ needs.load-testing.result }}\n- Stress Testing: ${{ needs.stress-testing.result }}\n- Resource Testing: ${{ needs.resource-testing.result }}\n- Concurrent Testing: ${{ needs.concurrent-testing.result }}\n\nCheck the artifacts for detailed reports.`
            })

  # Cleanup
  cleanup:
    name: ğŸ§¹ Cleanup
    runs-on: ubuntu-latest
    needs: [generate-performance-report]
    if: always()

    steps:
      - name: ğŸ§¹ Cleanup Test Environment
        run: |
          echo "ğŸ§¹ Cleaning up test environment..."
          docker-compose down -v || true
          docker system prune -f || true
