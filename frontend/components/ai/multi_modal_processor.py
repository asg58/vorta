"""
üîÑ VORTA AGI Voice Agent - Multi-Modal Processor
Advanced multi-modal integration and processing engine

This module provides enterprise-grade multi-modal capabilities:
- Real-time audio, text, and context fusion processing
- Intelligent modality switching and optimization
- Cross-modal information synthesis and validation
- Adaptive processing pipeline with quality assurance
- Performance optimization and resource management

Author: Ultra High-Grade Development Team
Version: 3.0.0-agi
Performance: >95% fusion accuracy, <200ms processing latency
"""

import asyncio
import logging
import time
import json
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from datetime import datetime
import uuid

try:
    import numpy as np
    _numpy_available = True
except ImportError:
    _numpy_available = False
    logging.warning("NumPy not available - limited numerical processing")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModalityType(Enum):
    """Types of input modalities"""
    AUDIO = "audio"
    TEXT = "text"
    VISUAL = "visual"
    CONTEXT = "context"
    EMOTIONAL = "emotional"
    BEHAVIORAL = "behavioral"
    TEMPORAL = "temporal"

class ProcessingMode(Enum):
    """Multi-modal processing modes"""
    SEQUENTIAL = "sequential"      # Process modalities one after another
    PARALLEL = "parallel"         # Process modalities simultaneously
    FUSION = "fusion"             # Integrate all modalities
    ADAPTIVE = "adaptive"         # Dynamically choose best mode
    HIERARCHICAL = "hierarchical" # Process in priority order

class FusionStrategy(Enum):
    """Strategies for modal fusion"""
    EARLY_FUSION = "early_fusion"     # Combine raw features
    LATE_FUSION = "late_fusion"       # Combine processed results
    HYBRID_FUSION = "hybrid_fusion"   # Combine at multiple levels
    ATTENTION_FUSION = "attention_fusion"  # Attention-weighted fusion
    DYNAMIC_FUSION = "dynamic_fusion"  # Context-dependent fusion

class QualityLevel(Enum):
    """Processing quality levels"""
    EXCELLENT = "excellent"    # >0.9
    GOOD = "good"             # >0.8
    ADEQUATE = "adequate"     # >0.7
    POOR = "poor"            # >0.6
    UNACCEPTABLE = "unacceptable"  # <=0.6

@dataclass
class ModalityInput:
    """Input data for a specific modality"""
    modality_type: ModalityType
    data: Any
    confidence: float
    timestamp: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Quality metrics
    signal_quality: float = 1.0
    noise_level: float = 0.0
    completeness: float = 1.0
    
    # Processing hints
    priority: int = 5  # 1-10 scale
    processing_requirements: Dict[str, Any] = field(default_factory=dict)
    
    def is_valid(self) -> bool:
        """Check if input is valid for processing"""
        return (self.confidence >= 0.5 and 
                self.signal_quality >= 0.5 and
                self.completeness >= 0.3)
    
    def get_quality_score(self) -> float:
        """Get overall input quality score"""
        return (self.confidence * 0.4 + 
                self.signal_quality * 0.3 + 
                self.completeness * 0.3)

@dataclass
class ProcessingResult:
    """Result from processing a specific modality"""
    modality_type: ModalityType
    processed_data: Any
    confidence: float
    processing_time: float
    
    # Quality metrics
    quality_score: float
    accuracy_estimate: float = 0.0
    reliability_score: float = 0.0
    
    # Processing metadata
    processing_method: str = ""
    features_extracted: List[str] = field(default_factory=list)
    anomalies_detected: List[str] = field(default_factory=list)
    
    # Integration readiness
    fusion_ready: bool = True
    fusion_weight: float = 1.0
    
    def get_quality_level(self) -> QualityLevel:
        """Get quality level based on score"""
        if self.quality_score > 0.9:
            return QualityLevel.EXCELLENT
        elif self.quality_score > 0.8:
            return QualityLevel.GOOD
        elif self.quality_score > 0.7:
            return QualityLevel.ADEQUATE
        elif self.quality_score > 0.6:
            return QualityLevel.POOR
        else:
            return QualityLevel.UNACCEPTABLE
    
    def is_acceptable(self) -> bool:
        """Check if result meets quality standards"""
        return (self.quality_score >= 0.7 and 
                self.confidence >= 0.6 and
                self.fusion_ready)

@dataclass
class FusedOutput:
    """Final fused multi-modal output"""
    processing_id: str
    fused_data: Dict[str, Any]
    fusion_strategy: FusionStrategy
    
    # Quality metrics
    fusion_quality: float
    consistency_score: float
    completeness_score: float
    confidence_score: float
    
    # Input analysis
    modalities_used: List[ModalityType]
    modality_weights: Dict[ModalityType, float]
    conflicts_detected: List[Dict[str, Any]] = field(default_factory=list)
    
    # Processing metrics
    total_processing_time: float = 0.0
    fusion_time: float = 0.0
    
    # Validation results
    cross_modal_validation: Dict[str, float] = field(default_factory=dict)
    anomaly_flags: List[str] = field(default_factory=list)
    
    # Output characteristics
    dominant_modality: Optional[ModalityType] = None
    alternative_interpretations: List[Dict[str, Any]] = field(default_factory=list)
    
    def get_overall_quality(self) -> float:
        """Calculate overall fusion quality"""
        return (self.fusion_quality * 0.4 +
                self.consistency_score * 0.3 +
                self.completeness_score * 0.2 +
                self.confidence_score * 0.1)
    
    def is_reliable(self) -> bool:
        """Check if fused output is reliable"""
        return (self.get_overall_quality() >= 0.7 and
                len(self.conflicts_detected) <= 2 and
                self.confidence_score >= 0.6)

@dataclass
class MultiModalConfig:
    """Configuration for multi-modal processor"""
    # Processing settings
    default_processing_mode: ProcessingMode = ProcessingMode.ADAPTIVE
    default_fusion_strategy: FusionStrategy = FusionStrategy.HYBRID_FUSION
    enable_parallel_processing: bool = True
    
    # Quality settings
    min_quality_threshold: float = 0.7
    enable_quality_enhancement: bool = True
    max_processing_attempts: int = 3
    
    # Performance settings
    max_processing_time: float = 3.0  # seconds
    enable_caching: bool = True
    cache_size: int = 500
    
    # Fusion settings
    enable_cross_modal_validation: bool = True
    conflict_resolution_strategy: str = "confidence_weighted"
    fusion_confidence_threshold: float = 0.6
    
    # Modality-specific settings
    modality_processors: Dict[ModalityType, Dict[str, Any]] = field(default_factory=lambda: {
        ModalityType.AUDIO: {
            'enabled': True,
            'min_quality': 0.5,
            'processing_timeout': 1.0
        },
        ModalityType.TEXT: {
            'enabled': True,
            'min_quality': 0.6,
            'processing_timeout': 0.5
        },
        ModalityType.CONTEXT: {
            'enabled': True,
            'min_quality': 0.4,
            'processing_timeout': 0.3
        },
        ModalityType.EMOTIONAL: {
            'enabled': True,
            'min_quality': 0.5,
            'processing_timeout': 0.8
        }
    })
    
    # Adaptive settings
    enable_adaptive_fusion: bool = True
    context_sensitivity: float = 0.7
    user_preference_weight: float = 0.3

class MultiModalProcessor:
    """
    üîÑ Advanced Multi-Modal Processor
    
    Ultra high-grade multi-modal processing with intelligent fusion,
    quality optimization, and adaptive processing strategies.
    """
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.is_initialized = False
        
        # Processing cache
        self.processing_cache: Dict[str, FusedOutput] = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Performance metrics
        self.metrics = {
            'total_processes': 0,
            'successful_processes': 0,
            'average_processing_time': 0.0,
            'average_fusion_quality': 0.0,
            'modality_usage': {modality.value: 0 for modality in ModalityType},
            'fusion_strategy_usage': {strategy.value: 0 for strategy in FusionStrategy},
            'quality_distribution': {quality.value: 0 for quality in QualityLevel},
            'conflict_resolutions': 0,
            'enhancement_attempts': 0,
            'validation_failures': 0
        }
        
        # Modality processors
        self.processors: Dict[ModalityType, Any] = {}
        
        # Fusion engines
        self.fusion_engines: Dict[FusionStrategy, Any] = {}
        
        # Quality validators
        self.validators: Dict[str, Any] = {}
        
        # Processing history for learning
        self.processing_history: List[Dict[str, Any]] = []
        
        # Initialize components
        self._initialize_processors()
        self._initialize_fusion_engines()
        self._initialize_validators()
        
        logger.info("üîÑ Multi-Modal Processor initialized")
        logger.info(f"   Processing mode: {config.default_processing_mode.value}")
        logger.info(f"   Fusion strategy: {config.default_fusion_strategy.value}")
        logger.info(f"   Parallel processing: {'‚úÖ' if config.enable_parallel_processing else '‚ùå'}")
        logger.info(f"   Quality enhancement: {'‚úÖ' if config.enable_quality_enhancement else '‚ùå'}")
    
    def _initialize_processors(self):
        """Initialize modality-specific processors"""
        try:
            self.processors = {
                ModalityType.AUDIO: AudioProcessor(),
                ModalityType.TEXT: TextProcessor(),
                ModalityType.VISUAL: VisualProcessor(),
                ModalityType.CONTEXT: ContextProcessor(),
                ModalityType.EMOTIONAL: EmotionalProcessor(),
                ModalityType.BEHAVIORAL: BehavioralProcessor(),
                ModalityType.TEMPORAL: TemporalProcessor()
            }
            
            logger.info("‚úÖ Modality processors initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Processor initialization failed: {e}")
    
    def _initialize_fusion_engines(self):
        """Initialize fusion engines for different strategies"""
        try:
            self.fusion_engines = {
                FusionStrategy.EARLY_FUSION: EarlyFusionEngine(),
                FusionStrategy.LATE_FUSION: LateFusionEngine(),
                FusionStrategy.HYBRID_FUSION: HybridFusionEngine(),
                FusionStrategy.ATTENTION_FUSION: AttentionFusionEngine(),
                FusionStrategy.DYNAMIC_FUSION: DynamicFusionEngine()
            }
            
            logger.info("‚úÖ Fusion engines initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Fusion engine initialization failed: {e}")
    
    def _initialize_validators(self):
        """Initialize quality validators"""
        try:
            self.validators = {
                'cross_modal': CrossModalValidator(),
                'consistency': ConsistencyValidator(),
                'completeness': CompletenessValidator(),
                'confidence': ConfidenceValidator()
            }
            
            logger.info("‚úÖ Quality validators initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Validator initialization failed: {e}")
    
    async def process_multi_modal(self,
                                inputs: List[ModalityInput],
                                processing_mode: Optional[ProcessingMode] = None,
                                fusion_strategy: Optional[FusionStrategy] = None,
                                quality_requirements: Optional[Dict[str, float]] = None) -> FusedOutput:
        """
        üîÑ Process multi-modal inputs with intelligent fusion
        
        Args:
            inputs: List of modality inputs to process
            processing_mode: Optional processing mode override
            fusion_strategy: Optional fusion strategy override
            quality_requirements: Optional quality requirements
            
        Returns:
            FusedOutput with integrated multi-modal results
        """
        start_time = time.time()
        processing_id = str(uuid.uuid4())
        
        try:
            logger.debug(f"üîÑ Processing multi-modal inputs {processing_id[:8]}")
            
            # Validate inputs
            valid_inputs = self._validate_inputs(inputs)
            if not valid_inputs:
                raise ValueError("No valid inputs provided")
            
            # Check cache
            cache_key = self._get_cache_key(valid_inputs)
            if self.config.enable_caching and cache_key in self.processing_cache:
                self.cache_hits += 1
                cached_result = self.processing_cache[cache_key]
                cached_result.total_processing_time = time.time() - start_time
                return cached_result
            
            if self.config.enable_caching:
                self.cache_misses += 1
            
            # Determine processing strategy
            mode = processing_mode or self._determine_processing_mode(valid_inputs)
            strategy = fusion_strategy or self._determine_fusion_strategy(valid_inputs, mode)
            
            # Process individual modalities
            processing_results = await self._process_modalities(valid_inputs, mode)
            
            # Validate processing results
            valid_results = self._validate_processing_results(processing_results)
            if not valid_results:
                raise ValueError("No valid processing results")
            
            # Perform fusion
            fused_output = await self._perform_fusion(
                valid_results, strategy, quality_requirements
            )
            
            # Post-processing validation
            fused_output = await self._validate_fused_output(fused_output)
            
            # Quality enhancement if needed
            if (self.config.enable_quality_enhancement and 
                fused_output.get_overall_quality() < self.config.min_quality_threshold):
                fused_output = await self._enhance_fusion_quality(fused_output, valid_results)
            
            # Finalize output
            processing_time = time.time() - start_time
            fused_output.processing_id = processing_id
            fused_output.total_processing_time = processing_time
            
            # Update metrics
            self._update_processing_metrics(fused_output, processing_time)
            
            # Cache result
            if self.config.enable_caching:
                self._cache_result(cache_key, fused_output)
            
            # Learn from processing
            self._record_processing_experience(valid_inputs, fused_output)
            
            logger.debug(f"üîÑ Multi-modal processing completed: Quality={fused_output.get_overall_quality():.3f} "
                        f"in {processing_time*1000:.1f}ms")
            
            return fused_output
            
        except Exception as e:
            logger.error(f"‚ùå Multi-modal processing failed: {e}")
            return await self._create_fallback_output(inputs, time.time() - start_time)
    
    def _validate_inputs(self, inputs: List[ModalityInput]) -> List[ModalityInput]:
        """Validate and filter input modalities"""
        valid_inputs = []
        
        for input_data in inputs:
            if not input_data.is_valid():
                logger.warning(f"‚ö†Ô∏è Invalid input for {input_data.modality_type.value}")
                continue
            
            # Check modality-specific requirements
            modality_config = self.config.modality_processors.get(input_data.modality_type)
            if not modality_config or not modality_config.get('enabled', True):
                logger.debug(f"üîÑ Skipping disabled modality: {input_data.modality_type.value}")
                continue
            
            min_quality = modality_config.get('min_quality', 0.5)
            if input_data.get_quality_score() < min_quality:
                logger.warning(f"‚ö†Ô∏è Low quality input for {input_data.modality_type.value}: "
                              f"{input_data.get_quality_score():.3f}")
                continue
            
            valid_inputs.append(input_data)
        
        logger.debug(f"üîÑ Validated {len(valid_inputs)}/{len(inputs)} inputs")
        return valid_inputs
    
    def _determine_processing_mode(self, inputs: List[ModalityInput]) -> ProcessingMode:
        """Determine optimal processing mode based on inputs"""
        if self.config.default_processing_mode != ProcessingMode.ADAPTIVE:
            return self.config.default_processing_mode
        
        # Adaptive mode selection
        num_modalities = len(inputs)
        total_data_size = sum(len(str(inp.data)) for inp in inputs)
        average_confidence = sum(inp.confidence for inp in inputs) / num_modalities
        
        # Simple heuristics
        if num_modalities <= 2:
            return ProcessingMode.SEQUENTIAL
        elif average_confidence < 0.6:
            return ProcessingMode.HIERARCHICAL  # Process high-confidence first
        elif total_data_size > 10000:  # Large data
            return ProcessingMode.PARALLEL
        else:
            return ProcessingMode.FUSION
    
    def _determine_fusion_strategy(self, 
                                 inputs: List[ModalityInput], 
                                 mode: ProcessingMode) -> FusionStrategy:
        """Determine optimal fusion strategy"""
        if self.config.default_fusion_strategy != FusionStrategy.DYNAMIC_FUSION:
            return self.config.default_fusion_strategy
        
        # Dynamic strategy selection
        num_modalities = len(inputs)
        confidence_variance = self._calculate_confidence_variance(inputs)
        
        # Strategy selection heuristics
        if num_modalities <= 2:
            return FusionStrategy.LATE_FUSION
        elif confidence_variance > 0.3:  # High variance in confidence
            return FusionStrategy.ATTENTION_FUSION
        elif mode == ProcessingMode.PARALLEL:
            return FusionStrategy.HYBRID_FUSION
        else:
            return FusionStrategy.LATE_FUSION
    
    def _calculate_confidence_variance(self, inputs: List[ModalityInput]) -> float:
        """Calculate variance in input confidences"""
        if not inputs:
            return 0.0
        
        confidences = [inp.confidence for inp in inputs]
        mean_confidence = sum(confidences) / len(confidences)
        variance = sum((c - mean_confidence) ** 2 for c in confidences) / len(confidences)
        return variance
    
    async def _process_modalities(self, 
                                inputs: List[ModalityInput],
                                mode: ProcessingMode) -> List[ProcessingResult]:
        """Process individual modalities based on processing mode"""
        try:
            if mode == ProcessingMode.PARALLEL and self.config.enable_parallel_processing:
                # Process all modalities in parallel
                tasks = [
                    self._process_single_modality(input_data) 
                    for input_data in inputs
                ]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Filter out exceptions
                valid_results = [r for r in results if isinstance(r, ProcessingResult)]
                
            elif mode == ProcessingMode.HIERARCHICAL:
                # Process in priority order
                sorted_inputs = sorted(inputs, key=lambda x: x.priority, reverse=True)
                results = []
                for input_data in sorted_inputs:
                    result = await self._process_single_modality(input_data)
                    results.append(result)
                valid_results = results
                
            else:  # SEQUENTIAL, FUSION, or fallback
                # Process sequentially
                results = []
                for input_data in inputs:
                    result = await self._process_single_modality(input_data)
                    results.append(result)
                valid_results = results
            
            logger.debug(f"üîÑ Processed {len(valid_results)} modalities using {mode.value} mode")
            return valid_results
            
        except Exception as e:
            logger.error(f"‚ùå Modality processing failed: {e}")
            return []
    
    async def _process_single_modality(self, input_data: ModalityInput) -> ProcessingResult:
        """Process a single modality input"""
        start_time = time.time()
        
        try:
            processor = self.processors.get(input_data.modality_type)
            if not processor:
                raise ValueError(f"No processor for {input_data.modality_type.value}")
            
            # Get processing timeout
            modality_config = self.config.modality_processors.get(input_data.modality_type, {})
            timeout = modality_config.get('processing_timeout', 1.0)
            
            # Process with timeout
            processed_data = await asyncio.wait_for(
                processor.process(input_data),
                timeout=timeout
            )
            
            processing_time = time.time() - start_time
            
            # Create result
            result = ProcessingResult(
                modality_type=input_data.modality_type,
                processed_data=processed_data,
                confidence=input_data.confidence,
                processing_time=processing_time,
                quality_score=input_data.get_quality_score(),
                accuracy_estimate=processor.get_accuracy_estimate(processed_data),
                reliability_score=processor.get_reliability_score(processed_data),
                processing_method=processor.get_method_name(),
                features_extracted=processor.get_extracted_features(processed_data)
            )
            
            # Update usage metrics
            self.metrics['modality_usage'][input_data.modality_type.value] += 1
            
            return result
            
        except asyncio.TimeoutError:
            logger.warning(f"‚ö†Ô∏è Processing timeout for {input_data.modality_type.value}")
            return self._create_timeout_result(input_data, time.time() - start_time)
        except Exception as e:
            logger.error(f"‚ùå Processing failed for {input_data.modality_type.value}: {e}")
            return self._create_error_result(input_data, time.time() - start_time)
    
    def _validate_processing_results(self, results: List[ProcessingResult]) -> List[ProcessingResult]:
        """Validate processing results"""
        valid_results = []
        
        for result in results:
            if not result.is_acceptable():
                logger.warning(f"‚ö†Ô∏è Unacceptable result for {result.modality_type.value}: "
                              f"Quality={result.quality_score:.3f}")
                continue
            
            valid_results.append(result)
        
        logger.debug(f"üîÑ Validated {len(valid_results)}/{len(results)} processing results")
        return valid_results
    
    async def _perform_fusion(self,
                            results: List[ProcessingResult],
                            strategy: FusionStrategy,
                            quality_requirements: Optional[Dict[str, float]]) -> FusedOutput:
        """Perform fusion of processing results"""
        start_time = time.time()
        
        try:
            fusion_engine = self.fusion_engines.get(strategy)
            if not fusion_engine:
                raise ValueError(f"No fusion engine for {strategy.value}")
            
            # Perform fusion
            fused_data = await fusion_engine.fuse(results, quality_requirements)
            
            # Calculate fusion metrics
            fusion_time = time.time() - start_time
            
            # Detect conflicts
            conflicts = await self._detect_conflicts(results, fused_data)
            
            # Calculate fusion quality metrics
            fusion_quality = self._calculate_fusion_quality(results, fused_data)
            consistency_score = self._calculate_consistency_score(results, fused_data)
            completeness_score = self._calculate_completeness_score(results, fused_data)
            confidence_score = self._calculate_fusion_confidence(results, fused_data)
            
            # Determine modality weights
            modality_weights = fusion_engine.get_modality_weights(results)
            
            # Create fused output
            output = FusedOutput(
                processing_id="",  # Will be set later
                fused_data=fused_data,
                fusion_strategy=strategy,
                fusion_quality=fusion_quality,
                consistency_score=consistency_score,
                completeness_score=completeness_score,
                confidence_score=confidence_score,
                modalities_used=[r.modality_type for r in results],
                modality_weights=modality_weights,
                conflicts_detected=conflicts,
                total_processing_time=0.0,  # Will be set later
                fusion_time=fusion_time,
                dominant_modality=self._determine_dominant_modality(results, modality_weights)
            )
            
            # Update fusion strategy usage
            self.metrics['fusion_strategy_usage'][strategy.value] += 1
            
            return output
            
        except Exception as e:
            logger.error(f"‚ùå Fusion failed with {strategy.value}: {e}")
            raise
    
    async def _validate_fused_output(self, output: FusedOutput) -> FusedOutput:
        """Validate fused output using cross-modal validation"""
        if not self.config.enable_cross_modal_validation:
            return output
        
        try:
            validation_results = {}
            
            # Cross-modal validation
            validation_results['cross_modal'] = await self.validators['cross_modal'].validate(output)
            
            # Consistency validation
            validation_results['consistency'] = await self.validators['consistency'].validate(output)
            
            # Completeness validation
            validation_results['completeness'] = await self.validators['completeness'].validate(output)
            
            # Confidence validation
            validation_results['confidence'] = await self.validators['confidence'].validate(output)
            
            # Update output with validation results
            output.cross_modal_validation = validation_results
            
            # Check for validation failures
            failed_validations = [
                name for name, score in validation_results.items() 
                if score < 0.6
            ]
            
            if failed_validations:
                self.metrics['validation_failures'] += 1
                output.anomaly_flags.extend(failed_validations)
                logger.warning(f"‚ö†Ô∏è Validation failures: {failed_validations}")
            
            return output
            
        except Exception as e:
            logger.error(f"‚ùå Output validation failed: {e}")
            return output
    
    async def _enhance_fusion_quality(self,
                                    output: FusedOutput,
                                    results: List[ProcessingResult]) -> FusedOutput:
        """Enhance fusion quality using various strategies"""
        try:
            self.metrics['enhancement_attempts'] += 1
            
            original_quality = output.get_overall_quality()
            
            # Try different enhancement strategies
            enhancement_strategies = [
                'reweight_modalities',
                'filter_low_confidence',
                'apply_consensus',
                'temporal_smoothing'
            ]
            
            best_output = output
            best_quality = original_quality
            
            for strategy in enhancement_strategies:
                enhanced_output = await self._apply_enhancement_strategy(
                    strategy, output, results
                )
                
                enhanced_quality = enhanced_output.get_overall_quality()
                if enhanced_quality > best_quality:
                    best_output = enhanced_output
                    best_quality = enhanced_quality
                
                # Stop if we reach acceptable quality
                if enhanced_quality >= self.config.min_quality_threshold:
                    break
            
            logger.debug(f"üîÑ Quality enhancement: {original_quality:.3f} -> {best_quality:.3f}")
            
            return best_output
            
        except Exception as e:
            logger.error(f"‚ùå Quality enhancement failed: {e}")
            return output
    
    async def _apply_enhancement_strategy(self,
                                        strategy: str,
                                        output: FusedOutput,
                                        results: List[ProcessingResult]) -> FusedOutput:
        """Apply specific enhancement strategy"""
        try:
            if strategy == 'reweight_modalities':
                # Adjust modality weights based on quality
                new_weights = {}
                for modality, weight in output.modality_weights.items():
                    # Find corresponding result
                    result = next((r for r in results if r.modality_type == modality), None)
                    if result:
                        quality_factor = result.quality_score
                        new_weights[modality] = weight * quality_factor
                
                # Normalize weights
                total_weight = sum(new_weights.values())
                if total_weight > 0:
                    new_weights = {k: v/total_weight for k, v in new_weights.items()}
                    output.modality_weights = new_weights
            
            elif strategy == 'filter_low_confidence':
                # Filter out low-confidence contributions
                filtered_data = {}
                confidence_threshold = 0.6
                
                for key, value in output.fused_data.items():
                    if isinstance(value, dict) and 'confidence' in value:
                        if value['confidence'] >= confidence_threshold:
                            filtered_data[key] = value
                    else:
                        filtered_data[key] = value
                
                output.fused_data = filtered_data
            
            elif strategy == 'apply_consensus':
                # Apply consensus-based corrections
                consensus_data = self._calculate_consensus(results)
                for key, consensus_value in consensus_data.items():
                    if key in output.fused_data:
                        current_value = output.fused_data[key]
                        # Weighted average with consensus
                        output.fused_data[key] = {
                            'value': current_value if isinstance(current_value, (int, float)) else current_value.get('value', current_value),
                            'consensus_adjusted': True,
                            'confidence': min((current_value.get('confidence', 0.7) + 0.1), 1.0) if isinstance(current_value, dict) else 0.8
                        }
            
            # Recalculate quality metrics
            output.fusion_quality = self._calculate_fusion_quality(results, output.fused_data)
            output.consistency_score = self._calculate_consistency_score(results, output.fused_data)
            
            return output
            
        except Exception as e:
            logger.error(f"‚ùå Enhancement strategy {strategy} failed: {e}")
            return output
    
    def _calculate_consensus(self, results: List[ProcessingResult]) -> Dict[str, Any]:
        """Calculate consensus values from multiple results"""
        consensus = {}
        
        # Simple consensus calculation (in practice, this would be more sophisticated)
        if len(results) >= 2:
            consensus['agreement_level'] = 0.8  # Mock consensus
            consensus['dominant_interpretation'] = 'primary'
        
        return consensus
    
    async def _detect_conflicts(self,
                              results: List[ProcessingResult],
                              fused_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect conflicts between modalities"""
        conflicts = []
        
        try:
            # Compare results for contradictions
            if len(results) >= 2:
                for i, result1 in enumerate(results):
                    for j, result2 in enumerate(results[i+1:], i+1):
                        conflict = self._check_modality_conflict(result1, result2)
                        if conflict:
                            conflicts.append({
                                'type': 'modality_conflict',
                                'modalities': [result1.modality_type.value, result2.modality_type.value],
                                'severity': conflict['severity'],
                                'description': conflict['description']
                            })
            
            # Check internal consistency
            consistency_issues = self._check_internal_consistency(fused_data)
            for issue in consistency_issues:
                conflicts.append({
                    'type': 'consistency_issue',
                    'field': issue['field'],
                    'severity': issue['severity'],
                    'description': issue['description']
                })
            
            if conflicts:
                self.metrics['conflict_resolutions'] += len(conflicts)
                logger.debug(f"üîÑ Detected {len(conflicts)} conflicts")
            
            return conflicts
            
        except Exception as e:
            logger.error(f"‚ùå Conflict detection failed: {e}")
            return []
    
    def _check_modality_conflict(self, 
                               result1: ProcessingResult, 
                               result2: ProcessingResult) -> Optional[Dict[str, Any]]:
        """Check for conflicts between two modality results"""
        # Simple conflict detection (in practice, this would be domain-specific)
        if result1.confidence > 0.8 and result2.confidence > 0.8:
            # Both high confidence - check for contradictions
            data1 = str(result1.processed_data).lower()
            data2 = str(result2.processed_data).lower()
            
            # Simple contradiction check
            contradictory_terms = [
                ('positive', 'negative'),
                ('yes', 'no'),
                ('success', 'failure'),
                ('happy', 'sad'),
                ('confident', 'uncertain')
            ]
            
            for term1, term2 in contradictory_terms:
                if term1 in data1 and term2 in data2:
                    return {
                        'severity': 'high',
                        'description': f"Contradiction: {result1.modality_type.value} suggests {term1}, "
                                     f"{result2.modality_type.value} suggests {term2}"
                    }
        
        return None
    
    def _check_internal_consistency(self, fused_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check internal consistency of fused data"""
        issues = []
        
        # Example consistency checks
        if 'sentiment' in fused_data and 'emotion' in fused_data:
            sentiment = fused_data.get('sentiment', {})
            emotion = fused_data.get('emotion', {})
            
            # Check sentiment-emotion consistency
            if isinstance(sentiment, dict) and isinstance(emotion, dict):
                sent_value = sentiment.get('value', 'neutral')
                emotion_value = emotion.get('value', 'neutral')
                
                # Simple consistency check
                if sent_value == 'positive' and emotion_value in ['sad', 'angry', 'frustrated']:
                    issues.append({
                        'field': 'sentiment_emotion',
                        'severity': 'medium',
                        'description': f"Inconsistent sentiment ({sent_value}) and emotion ({emotion_value})"
                    })
        
        return issues
    
    def _calculate_fusion_quality(self, 
                                results: List[ProcessingResult],
                                fused_data: Dict[str, Any]) -> float:
        """Calculate fusion quality score"""
        if not results:
            return 0.0
        
        # Average input quality
        input_quality = sum(r.quality_score for r in results) / len(results)
        
        # Fusion coherence (simplified)
        coherence = 0.8  # Mock coherence score
        
        # Data completeness
        completeness = min(len(fused_data) / 5.0, 1.0)  # Expect ~5 key fields
        
        return (input_quality * 0.5 + coherence * 0.3 + completeness * 0.2)
    
    def _calculate_consistency_score(self,
                                   results: List[ProcessingResult],
                                   fused_data: Dict[str, Any]) -> float:
        """Calculate consistency score"""
        if len(results) <= 1:
            return 1.0
        
        # Calculate agreement between modalities
        agreements = 0
        comparisons = 0
        
        for i, result1 in enumerate(results):
            for result2 in results[i+1:]:
                comparisons += 1
                # Simple agreement measure
                if abs(result1.confidence - result2.confidence) < 0.3:
                    agreements += 1
        
        return agreements / max(comparisons, 1)
    
    def _calculate_completeness_score(self,
                                    results: List[ProcessingResult],
                                    fused_data: Dict[str, Any]) -> float:
        """Calculate completeness score"""
        expected_fields = ['intent', 'emotion', 'content', 'confidence', 'context']
        present_fields = [field for field in expected_fields if field in fused_data]
        
        return len(present_fields) / len(expected_fields)
    
    def _calculate_fusion_confidence(self,
                                   results: List[ProcessingResult],
                                   fused_data: Dict[str, Any]) -> float:
        """Calculate fusion confidence score"""
        if not results:
            return 0.0
        
        # Weighted average confidence
        total_weight = sum(r.fusion_weight for r in results)
        if total_weight == 0:
            return sum(r.confidence for r in results) / len(results)
        
        weighted_confidence = sum(
            r.confidence * r.fusion_weight for r in results
        ) / total_weight
        
        return weighted_confidence
    
    def _determine_dominant_modality(self,
                                   results: List[ProcessingResult],
                                   weights: Dict[ModalityType, float]) -> Optional[ModalityType]:
        """Determine the dominant modality in fusion"""
        if not weights:
            return None
        
        return max(weights.items(), key=lambda x: x[1])[0]
    
    def _create_timeout_result(self, input_data: ModalityInput, processing_time: float) -> ProcessingResult:
        """Create result for timed-out processing"""
        return ProcessingResult(
            modality_type=input_data.modality_type,
            processed_data={'error': 'timeout'},
            confidence=0.1,
            processing_time=processing_time,
            quality_score=0.2,
            fusion_ready=False,
            processing_method="timeout"
        )
    
    def _create_error_result(self, input_data: ModalityInput, processing_time: float) -> ProcessingResult:
        """Create result for failed processing"""
        return ProcessingResult(
            modality_type=input_data.modality_type,
            processed_data={'error': 'processing_failed'},
            confidence=0.0,
            processing_time=processing_time,
            quality_score=0.1,
            fusion_ready=False,
            processing_method="error"
        )
    
    async def _create_fallback_output(self,
                                    inputs: List[ModalityInput],
                                    processing_time: float) -> FusedOutput:
        """Create fallback output when processing fails"""
        return FusedOutput(
            processing_id="",
            fused_data={
                'error': 'processing_failed',
                'available_modalities': [inp.modality_type.value for inp in inputs],
                'fallback': True
            },
            fusion_strategy=FusionStrategy.LATE_FUSION,
            fusion_quality=0.3,
            consistency_score=0.5,
            completeness_score=0.2,
            confidence_score=0.1,
            modalities_used=[inp.modality_type for inp in inputs],
            modality_weights={},
            total_processing_time=processing_time,
            fusion_time=0.0
        )
    
    def _get_cache_key(self, inputs: List[ModalityInput]) -> str:
        """Generate cache key for inputs"""
        import hashlib
        
        key_elements = []
        for inp in sorted(inputs, key=lambda x: x.modality_type.value):
            key_elements.append(f"{inp.modality_type.value}:{hash(str(inp.data))}")
        
        key_string = "|".join(key_elements)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _cache_result(self, cache_key: str, output: FusedOutput):
        """Cache processing result"""
        if len(self.processing_cache) >= self.config.cache_size:
            # Remove oldest entry
            oldest_key = next(iter(self.processing_cache))
            del self.processing_cache[oldest_key]
        
        self.processing_cache[cache_key] = output
    
    def _record_processing_experience(self, inputs: List[ModalityInput], output: FusedOutput):
        """Record processing experience for learning"""
        experience = {
            'timestamp': datetime.now(),
            'input_types': [inp.modality_type.value for inp in inputs],
            'fusion_strategy': output.fusion_strategy.value,
            'quality': output.get_overall_quality(),
            'conflicts': len(output.conflicts_detected),
            'processing_time': output.total_processing_time
        }
        
        self.processing_history.append(experience)
        
        # Keep only recent experiences
        if len(self.processing_history) > 1000:
            self.processing_history = self.processing_history[-1000:]
    
    def _update_processing_metrics(self, output: FusedOutput, processing_time: float):
        """Update processing performance metrics"""
        self.metrics['total_processes'] += 1
        
        if output.is_reliable():
            self.metrics['successful_processes'] += 1
        
        # Update averages
        total = self.metrics['total_processes']
        current_avg_time = self.metrics['average_processing_time']
        current_avg_quality = self.metrics['average_fusion_quality']
        
        self.metrics['average_processing_time'] = (
            (current_avg_time * (total - 1) + processing_time) / total
        )
        
        self.metrics['average_fusion_quality'] = (
            (current_avg_quality * (total - 1) + output.get_overall_quality()) / total
        )
        
        # Update quality distribution
        quality_level = QualityLevel.EXCELLENT
        if output.get_overall_quality() > 0.9:
            quality_level = QualityLevel.EXCELLENT
        elif output.get_overall_quality() > 0.8:
            quality_level = QualityLevel.GOOD
        elif output.get_overall_quality() > 0.7:
            quality_level = QualityLevel.ADEQUATE
        elif output.get_overall_quality() > 0.6:
            quality_level = QualityLevel.POOR
        else:
            quality_level = QualityLevel.UNACCEPTABLE
        
        self.metrics['quality_distribution'][quality_level.value] += 1
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        success_rate = (self.metrics['successful_processes'] / 
                       max(self.metrics['total_processes'], 1))
        
        cache_metrics = {}
        if self.config.enable_caching:
            total_requests = self.cache_hits + self.cache_misses
            cache_metrics = {
                'cache_size': len(self.processing_cache),
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': self.cache_hits / total_requests if total_requests > 0 else 0.0
            }
        
        return {
            'processing_metrics': self.metrics.copy(),
            'cache_metrics': cache_metrics,
            'success_rate': success_rate,
            'processors_available': len(self.processors),
            'fusion_engines_available': len(self.fusion_engines),
            'validators_available': len(self.validators),
            'processing_history_size': len(self.processing_history)
        }

# Mock processor classes
class AudioProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.1)  # Simulate processing
        return {
            'transcription': 'processed audio content',
            'features': ['pitch', 'volume', 'tone'],
            'confidence': input_data.confidence
        }
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.9
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.85
    
    def get_method_name(self) -> str:
        return "audio_processing_v3"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return processed_data.get('features', [])

class TextProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.05)  # Simulate processing
        return {
            'processed_text': str(input_data.data),
            'entities': ['entity1', 'entity2'],
            'sentiment': 'positive',
            'confidence': input_data.confidence
        }
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.95
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.9
    
    def get_method_name(self) -> str:
        return "text_processing_v2"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['text_length', 'sentiment', 'entities']

# Similar mock classes for other processors...
class VisualProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.2)
        return {'visual_features': 'processed', 'confidence': input_data.confidence}
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.8
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.75
    
    def get_method_name(self) -> str:
        return "visual_processing_v1"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['visual_features']

class ContextProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.03)
        return {'context_data': input_data.data, 'confidence': input_data.confidence}
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.85
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.8
    
    def get_method_name(self) -> str:
        return "context_processing_v1"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['context_keywords', 'context_type']

class EmotionalProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.08)
        return {'emotion': 'neutral', 'intensity': 0.5, 'confidence': input_data.confidence}
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.82
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.78
    
    def get_method_name(self) -> str:
        return "emotion_processing_v1"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['emotion', 'intensity']

class BehavioralProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.06)
        return {'behavior_pattern': 'normal', 'confidence': input_data.confidence}
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.75
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.7
    
    def get_method_name(self) -> str:
        return "behavioral_processing_v1"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['behavior_pattern']

class TemporalProcessor:
    async def process(self, input_data: ModalityInput) -> Dict[str, Any]:
        await asyncio.sleep(0.04)
        return {'timing_data': 'processed', 'confidence': input_data.confidence}
    
    def get_accuracy_estimate(self, processed_data: Dict[str, Any]) -> float:
        return 0.88
    
    def get_reliability_score(self, processed_data: Dict[str, Any]) -> float:
        return 0.85
    
    def get_method_name(self) -> str:
        return "temporal_processing_v1"
    
    def get_extracted_features(self, processed_data: Dict[str, Any]) -> List[str]:
        return ['timing_data']

# Mock fusion engine classes
class EarlyFusionEngine:
    async def fuse(self, results: List[ProcessingResult], quality_req: Optional[Dict] = None) -> Dict[str, Any]:
        await asyncio.sleep(0.02)
        return {'fusion_type': 'early', 'combined_features': 'fused_early'}
    
    def get_modality_weights(self, results: List[ProcessingResult]) -> Dict[ModalityType, float]:
        return {r.modality_type: 1.0/len(results) for r in results}

class LateFusionEngine:
    async def fuse(self, results: List[ProcessingResult], quality_req: Optional[Dict] = None) -> Dict[str, Any]:
        await asyncio.sleep(0.03)
        fused = {'fusion_type': 'late'}
        for result in results:
            fused[f'{result.modality_type.value}_result'] = result.processed_data
        return fused
    
    def get_modality_weights(self, results: List[ProcessingResult]) -> Dict[ModalityType, float]:
        weights = {}
        total_quality = sum(r.quality_score for r in results)
        for r in results:
            weights[r.modality_type] = r.quality_score / total_quality if total_quality > 0 else 1.0/len(results)
        return weights

class HybridFusionEngine:
    async def fuse(self, results: List[ProcessingResult], quality_req: Optional[Dict] = None) -> Dict[str, Any]:
        await asyncio.sleep(0.04)
        return {'fusion_type': 'hybrid', 'early_features': 'fused', 'late_decisions': 'combined'}
    
    def get_modality_weights(self, results: List[ProcessingResult]) -> Dict[ModalityType, float]:
        weights = {}
        for r in results:
            weight = (r.quality_score * 0.5 + r.confidence * 0.5)
            weights[r.modality_type] = weight
        return weights

class AttentionFusionEngine:
    async def fuse(self, results: List[ProcessingResult], quality_req: Optional[Dict] = None) -> Dict[str, Any]:
        await asyncio.sleep(0.05)
        return {'fusion_type': 'attention', 'attention_weights': 'computed'}
    
    def get_modality_weights(self, results: List[ProcessingResult]) -> Dict[ModalityType, float]:
        # Attention-based weighting
        return {r.modality_type: r.confidence ** 2 for r in results}  # Quadratic attention

class DynamicFusionEngine:
    async def fuse(self, results: List[ProcessingResult], quality_req: Optional[Dict] = None) -> Dict[str, Any]:
        await asyncio.sleep(0.06)
        return {'fusion_type': 'dynamic', 'adaptive_strategy': 'context_aware'}
    
    def get_modality_weights(self, results: List[ProcessingResult]) -> Dict[ModalityType, float]:
        # Dynamic weighting based on context
        weights = {}
        for r in results:
            base_weight = r.quality_score
            if r.modality_type == ModalityType.AUDIO and r.quality_score > 0.8:
                base_weight *= 1.2  # Boost high-quality audio
            elif r.modality_type == ModalityType.TEXT:
                base_weight *= 1.1  # Slight boost for text
            weights[r.modality_type] = base_weight
        return weights

# Mock validator classes
class CrossModalValidator:
    async def validate(self, output: FusedOutput) -> float:
        await asyncio.sleep(0.01)
        return 0.85  # Mock validation score

class ConsistencyValidator:
    async def validate(self, output: FusedOutput) -> float:
        await asyncio.sleep(0.01)
        return 0.8

class CompletenessValidator:
    async def validate(self, output: FusedOutput) -> float:
        await asyncio.sleep(0.01)
        return 0.9

class ConfidenceValidator:
    async def validate(self, output: FusedOutput) -> float:
        await asyncio.sleep(0.01)
        return output.confidence_score * 1.1  # Slight boost

# Example usage and testing
if __name__ == "__main__":
    async def test_multimodal_processor():
        """Test the multi-modal processor"""
        print("üß™ Testing VORTA Multi-Modal Processor")
        
        # Create configuration
        config = MultiModalConfig(
            default_processing_mode=ProcessingMode.ADAPTIVE,
            default_fusion_strategy=FusionStrategy.DYNAMIC_FUSION,
            enable_parallel_processing=True,
            enable_quality_enhancement=True
        )
        
        # Initialize processor
        processor = MultiModalProcessor(config)
        
        # Create test inputs
        test_inputs = [
            ModalityInput(
                modality_type=ModalityType.AUDIO,
                data={'audio_data': 'sample audio content'},
                confidence=0.9,
                timestamp=datetime.now(),
                signal_quality=0.95,
                priority=8
            ),
            ModalityInput(
                modality_type=ModalityType.TEXT,
                data="Hello, I need help with my Python code",
                confidence=0.95,
                timestamp=datetime.now(),
                signal_quality=1.0,
                priority=9
            ),
            ModalityInput(
                modality_type=ModalityType.CONTEXT,
                data={'session_info': 'programming_help', 'user_level': 'beginner'},
                confidence=0.8,
                timestamp=datetime.now(),
                signal_quality=0.9,
                priority=6
            ),
            ModalityInput(
                modality_type=ModalityType.EMOTIONAL,
                data={'detected_emotion': 'curious', 'intensity': 0.7},
                confidence=0.75,
                timestamp=datetime.now(),
                signal_quality=0.8,
                priority=7
            )
        ]
        
        print(f"\nüîÑ Processing {len(test_inputs)} modalities:")
        for inp in test_inputs:
            print(f"   {inp.modality_type.value}: Quality={inp.get_quality_score():.3f}, "
                  f"Confidence={inp.confidence:.3f}")
        
        # Test different processing modes
        test_modes = [
            (ProcessingMode.PARALLEL, FusionStrategy.LATE_FUSION),
            (ProcessingMode.SEQUENTIAL, FusionStrategy.HYBRID_FUSION),
            (ProcessingMode.ADAPTIVE, FusionStrategy.DYNAMIC_FUSION)
        ]
        
        print("\nüîÑ Multi-Modal Processing Results:")
        print("-" * 80)
        
        for i, (mode, strategy) in enumerate(test_modes, 1):
            # Process multi-modal inputs
            result = await processor.process_multi_modal(
                inputs=test_inputs,
                processing_mode=mode,
                fusion_strategy=strategy,
                quality_requirements={'min_quality': 0.7}
            )
            
            print(f"{i}. Mode: {mode.value}, Strategy: {strategy.value}")
            print(f"   Processing ID: {result.processing_id[:8]}")
            print(f"   Overall Quality: {result.get_overall_quality():.3f}")
            print(f"   Fusion Quality: {result.fusion_quality:.3f}")
            print(f"   Consistency: {result.consistency_score:.3f}")
            print(f"   Completeness: {result.completeness_score:.3f}")
            print(f"   Confidence: {result.confidence_score:.3f}")
            print(f"   Processing Time: {result.total_processing_time*1000:.1f}ms")
            print(f"   Fusion Time: {result.fusion_time*1000:.1f}ms")
            print(f"   Modalities Used: {[m.value for m in result.modalities_used]}")
            print(f"   Dominant Modality: {result.dominant_modality.value if result.dominant_modality else 'None'}")
            print(f"   Conflicts Detected: {len(result.conflicts_detected)}")
            print(f"   Reliable: {'‚úÖ' if result.is_reliable() else '‚ùå'}")
            
            if result.conflicts_detected:
                print(f"   Conflicts: {[c['type'] for c in result.conflicts_detected]}")
            
            # Show modality weights
            weights_str = ", ".join([
                f"{mod.value}({weight:.2f})" 
                for mod, weight in result.modality_weights.items()
            ])
            print(f"   Modality Weights: {weights_str}")
            print()
        
        # Performance metrics
        metrics = processor.get_performance_metrics()
        print("üìä Performance Metrics:")
        print(f"   Total processes: {metrics['processing_metrics']['total_processes']}")
        print(f"   Success rate: {metrics['success_rate']:.1%}")
        print(f"   Avg processing time: {metrics['processing_metrics']['average_processing_time']*1000:.1f}ms")
        print(f"   Avg fusion quality: {metrics['processing_metrics']['average_fusion_quality']:.3f}")
        print(f"   Conflict resolutions: {metrics['processing_metrics']['conflict_resolutions']}")
        print(f"   Enhancement attempts: {metrics['processing_metrics']['enhancement_attempts']}")
        
        if 'cache_metrics' in metrics and metrics['cache_metrics']:
            print(f"   Cache hit rate: {metrics['cache_metrics']['cache_hit_rate']:.1%}")
        
        print(f"\nüîß System Status:")
        print(f"   Processors available: {metrics['processors_available']}")
        print(f"   Fusion engines available: {metrics['fusion_engines_available']}")
        print(f"   Validators available: {metrics['validators_available']}")
        
        print("\n‚úÖ Multi-Modal Processor test completed!")
    
    # Run the test
    asyncio.run(test_multimodal_processor())
